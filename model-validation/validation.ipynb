{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation of the final model(s) per round and comparison to the wisdom of the crowd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "sys.path.append('auxiliary/')\n",
    "from data_processing import load_features, shape_data, shape_data_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose settings for the final model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_season = '2018-2019'  # hold-out season for validation\n",
    "level = 'match'  # match or team level features to use\n",
    "min_round_train = 5  # minimum number of first rounds to skip in every season (train set)\n",
    "min_round_test = 5  # minimum number of first rounds to skip in every season (test set)\n",
    "norm = True  # whether to normalise or not the features\n",
    "random_state = 10  # random state for the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose model hyper-parameters and feature sets for the models to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "     {'features': ['Position_x', 'Offence_x', 'Offence_y', 'Defence_y',\n",
    "                   'Diff_y', 'Home F4', 'Away F4'],\n",
    "      'n_estimators': 115, \n",
    "      'learning_rate': 0.7},\n",
    "     {'features': ['Position_x', 'Position_y', 'Offence_x', 'Offence_y',\n",
    "                   'Defence_y', 'Diff_y', 'Away F4'],\n",
    "      'n_estimators': 141, \n",
    "      'learning_rate': 0.7},\n",
    "     {'features': ['Position_x', 'Position_y', 'Offence_x', 'Offence_y',\n",
    "                   'Defence_x', 'Defence_y', 'form_x', 'form_y',\n",
    "                   'Diff_x', 'Diff_y', 'Home F4', 'Away F4'],\n",
    "      'n_estimators': 121, \n",
    "      'learning_rate': 1.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_features(level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Predict progressively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every week has each own model\n",
    "rounds = np.arange(2, 31, dtype=int)\n",
    "print('Rounds for validation:', rounds)\n",
    "accuracy = np.zeros((rounds.shape[0], len(params)))\n",
    "waccuracy = np.zeros((rounds.shape[0], len(params)))\n",
    "models_results = pd.DataFrame({'game_round': rounds.repeat(8)})\n",
    "for j, param in enumerate(tqdm(params)):\n",
    "    features = param['features']\n",
    "    n_estimators = param['n_estimators']\n",
    "    learning_rate = param['learning_rate']\n",
    "    model = AdaBoostClassifier(n_estimators=n_estimators, random_state=10,\n",
    "                               learning_rate=learning_rate)\n",
    "\n",
    "    y_pred_all = np.array([])\n",
    "    y_test_all = np.array([])\n",
    "    for i, game_round in enumerate(rounds):\n",
    "        train_inds = (df['Season'] != test_season) | ((df['Season'] == test_season) & (df['Round'] < game_round))\n",
    "        test_inds = ~ train_inds\n",
    "        X_train, y_train, df_train, _, scaler = shape_data_scaler(df[train_inds], features,\n",
    "                                                                  norm=norm, min_round=1)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        X_test, y_test, df_test, _, _ = shape_data_scaler(df[test_inds], features,\n",
    "                                                          norm=scaler, min_round=1)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accur = accuracy_score(y_test, y_pred)\n",
    "        w_accur = balanced_accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # store the predictions, actuals of the current round\n",
    "        y_pred_all = np.concatenate((y_pred_all, y_pred[:8]))\n",
    "        y_test_all = np.concatenate((y_test_all, y_test[:8]))\n",
    "\n",
    "        accuracy[i, j] = accur\n",
    "        waccuracy[i, j] = w_accur\n",
    "    \n",
    "    if 'actual' not in models_results.columns:\n",
    "        models_results['Actual'] = y_test_all.astype(int)\n",
    "    models_results['Pred_%d' % j] = y_pred_all.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_results['Pred_comb'] = np.where(models_results[['Pred_0', 'Pred_1', 'Pred_2']].sum(axis=1) > 1.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_results['Pred_Majority'] = np.zeros(models_results.shape[0], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [u for u in models_results.columns if u.startswith('Pred')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy scores')\n",
    "for col in model_list:\n",
    "    print('%s:' % col, \n",
    "          accuracy_score(models_results['Actual'], \n",
    "                         models_results[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Weighted accuracy scores')\n",
    "for col in model_list:\n",
    "    print('%s:' % col, \n",
    "          balanced_accuracy_score(models_results['Actual'], \n",
    "                                  models_results[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC-AUC scores')\n",
    "for col in model_list:\n",
    "    print('%s:' % col, roc_auc_score(models_results['Actual'], models_results[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Accuracy per round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_rounds = np.unique(models_results['game_round'].values)\n",
    "n_rounds = uniq_rounds.shape[0]\n",
    "round_accuracy = np.zeros(n_rounds)\n",
    "n_correct = np.zeros(n_rounds)\n",
    "for i, u in enumerate(uniq_rounds):\n",
    "    ii = models_results['game_round'] == u\n",
    "    n_correct[i] =  (models_results.loc[ii, 'Actual'].values == models_results.loc[ii, 'Pred_1'].values).sum()\n",
    "    round_accuracy[i] = accuracy_score(models_results.loc[ii, 'Actual'].values, models_results.loc[ii, 'Pred_1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = go.Bar(x=rounds, y=n_correct)\n",
    "layout = go.Layout(yaxis={'title': 'Number of Correctly Predicted Games'},\n",
    "                   xaxis={'title': 'Game Round'})\n",
    "fig = go.Figure(data, layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, interc, _, _, _ = linregress(uniq_rounds, round_accuracy)\n",
    "y = slope * uniq_rounds + interc\n",
    "data = [\n",
    "    go.Scatter(x=uniq_rounds, y=round_accuracy, mode='markers'),\n",
    "    go.Scatter(x=uniq_rounds, y=y)\n",
    "]\n",
    "layout = go.Layout(yaxis={'title': 'Accuracy'}, xaxis={'title': 'Game Round'}, showlegend=False)\n",
    "fig = go.Figure(data, layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wisdom of the Crowds\n",
    "The data for this task is available upon request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_files_pattern = os.path.expanduser('~/Documents/mia_syn_mia_app/output/2018-2019/predictions_day_%d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woc_results = np.array([])\n",
    "for i in rounds:\n",
    "    try:\n",
    "        woc_df = pd.read_csv(predict_files_pattern % i)\n",
    "        xx = woc_df[['game_%d' % u for u in range(1, 9)]].mode().values[0, :].flatten()\n",
    "    except:\n",
    "        print('File not found: round', i)\n",
    "        xx = np.full(8, np.nan)\n",
    "    woc_results = np.concatenate((woc_results, xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WoC predictions\n",
    "models_results['Pred_WoC'] = woc_results - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Pred_WoC' not in model_list:\n",
    "    model_list.append('Pred_WoC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of results without the missing round(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the missing round(s) (if any)\n",
    "ii = pd.notna(models_results['Pred_WoC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy Scores')\n",
    "for col in model_list:\n",
    "    print('%s: \\t' % col, \n",
    "          accuracy_score(models_results.loc[ii, 'Actual'].values,\n",
    "                         models_results.loc[ii, col].values)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Weighted-Accuracy Scores')\n",
    "for col in model_list:\n",
    "    print('%s: \\t' % col, \n",
    "          balanced_accuracy_score(models_results.loc[ii, 'Actual'].values, \n",
    "                                  models_results.loc[ii, col].values)\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
